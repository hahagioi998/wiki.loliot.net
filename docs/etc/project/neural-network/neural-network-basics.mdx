---
id: neural-network-basics
title: Neural Network Basics
sidebar_label: Basics
description: Neural Network Basics
keywords:
  - Neural Network
---

import useBaseUrl from "@docusaurus/useBaseUrl";

<link rel="stylesheet" href={useBaseUrl("katex/katex.min.css")} />

<center>
  <img src={useBaseUrl("img/etc/project/neural-network/neural-network.jpg")} />
</center>

# Neuron(Perceptron)

<center>
  <img
    src={useBaseUrl("img/etc/project/neural-network/neural-network-neuron.jpg")}
  />
</center>

$$
z^l_j = \sum_k{\omega^l_{jk}a^{l-1}_k} + b^l_j
$$

$$
a^l_j = \sigma\left(z^l_j\right)
$$

# Cost(Loss) function

## Quadratic cost function

$$
C \equiv \frac{1}{2} \lVert \mathbf{y} - \mathbf{a}^L \rVert^2 = \frac{1}{2} \sum_i{\left(y_i - a^L_i\right)^2}
$$

$$
C \geq 0 \quad \left(\mathbf{y}\text{ is the desired output}\right)
$$

# Neural Network Training

What we need to look for through neural network training are weights and biases to minimize the consequences of the cost function. When $\mathbf{w}$ is a vector representing weights and biases,

$$
C_{next} = C + \Delta C \approx C + \nabla C \cdot \Delta \mathbf{w}
$$

It must be $\nabla C \cdot \Delta \mathbf{w} < 0$, because $C$ should decrease. Therfore, $\Delta \mathbf{w}$ can be determined as

$$
\Delta \mathbf{w} = - \eta \nabla C = - \epsilon \frac{\nabla C}{\lVert \nabla C \rVert} \quad ( \epsilon > 0)
$$

$\eta$ is called **learning rate** and $\epsilon$ is called **step**. If the step is large, $C$ may diverge, and if the step is small, the convergence speed may be slow, so an appropriate value should be determined.

If $\Delta \mathbf{w}$ is determined, then $\mathbf{w}_{next}$ can be

$$
\mathbf{w}_{next} = \mathbf{w} + \Delta \mathbf{w}
$$

<center>
  <img
    src={useBaseUrl("img/etc/project/neural-network/learning-rate-loss.jpg")}
  />
</center>

## Stochastic gradient descent

$$
\nabla C = \frac{1}{n}\sum_x{\nabla C_x}
$$

When the number of training inputs is very large, this can take a long time. Stochastic gradient descent works by randomly picking out a small number $m$ of randomly chosen training inputs.

$$
\nabla C = \frac{1}{n}\sum_x{\nabla C_x} \approx \frac{1}{m}\sum^m_{i=1}{\nabla C_{X_i}}
$$

Those random training inputs $X_1, X_2, ..., X_m$ are called **mini-batch**.

## Back-propagation

<center>
  <img
    src={useBaseUrl(
      "img/etc/project/neural-network/neural-network-back-propagation.jpg"
    )}
  />
</center>

<br />

Back-propagation is used to find $\nabla C$, because it is difficult for a computer to obtain $\nabla C$ by differentiating Cost function.

Error $\delta^l_j$ of neuron $j$ in layer $l$ is defined as

$$
\delta^l_j \equiv \frac{\partial C}{\partial z^l_j}
$$

Since $z^l_j$ was obtained from inference, If we know $\mathbf{\delta}^{l+1}$, we can get $\delta^l_j$ as below.

$$
\begin{aligned}
\delta^l_j = \frac{\partial C}{\partial z^l_j}
& = \sum_i{\frac{\partial C}{\partial z^{l+1}_i} \frac{\partial z^{l+1}_i}{\partial z^l_j}}
  \quad \left( \frac{\partial z^{l+1}_i}{\partial z^l_j} = \omega^{l+1}_{ij} \, \sigma' \left(z^l_j\right) \right)\\
& = \sum_i{\frac{\partial C}{\partial z^{l+1}_i} \omega^{l+1}_{ij} \, \sigma' \left(z^l_j\right)} \\
& = \sum_i{\delta^{l+1}_i \omega^{l+1}_{ij} \, \sigma' \left(z^l_j\right)}
\end{aligned}
$$

Since $a^L_j$ was obtained from inference and $\delta^L_j = (a^L_j - y_j) \, \sigma' \left( z^L_j \right)$, we can get the errors like this:

$$
\delta^L_j = (a^L_j - y_j) \, \sigma' \left( z^L_j \right)
$$

$$
\delta^{L-1}_j = \sum_i{ \delta^L_i \omega^L_{ij} \, \sigma' \left(z^{L-1}_j\right)} \\
\vdots
$$

Finally, $\nabla C$ can be obtained by using the errors obtained above.

$$
\frac{\partial C}{\partial b^l_j} = \frac{\partial C}{\partial z^l_j} \frac{\partial z^l_j}{\partial b^l_j} = \delta^l_j
$$

$$
\frac{\partial C}{\partial \omega^l_{jk}} = \frac{\partial C}{\partial z^l_j} \frac{\partial z^l_j}{\partial \omega^l_{jk}} = \delta^l_j a^{l-1}_k
$$

## Training

Set initail weights and biases to random and repeat process **Inference -> Back-propagation -> weights and biases update**. When it is judged that $C$ cannot be made smaller, the final weights and biases are determined.

# Reference

- [http://neuralnetworksanddeeplearning.com](http://neuralnetworksanddeeplearning.com)
- [https://machinelearning.tobiashill.se](https://machinelearning.tobiashill.se)
- [http://www.bdhammel.com/learning-rates](http://www.bdhammel.com/learning-rates)
