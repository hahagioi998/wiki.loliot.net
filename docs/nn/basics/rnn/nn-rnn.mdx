---
id: nn-rnn
title: Recurrent Neural Network
sidebar_label: RNN
description: Neural Network RNN(Recurrent)
keywords:
  - Neural Network
  - Recurrent
---

import useBaseUrl from '@docusaurus/useBaseUrl';

## RNN(Recurrent Neural Network)

$$
input = Dim(batch\_size, timesteps, input\_size) \\
output = Dim(batch\_size, timesteps, hidden\_size)
$$

<center>
  <img src={useBaseUrl('img/nn/basics/rnn/nn-rnn-simple.jpg')} />
</center>

$$
h_t = \tanh(W_{hx} x_t + W_{hh} h_{t-1} + b_h)
$$

## Vanishing/exploding gradient

긴 시퀀스를 처리하는 RNN은 깊은 네트워크가 되면서 `Vanishing/exploding gradient` 문제가 발생하기 쉽습니다.

- Relu와 같이 수렴하지 않는 activation을 사용하면 불안정해질 수 있습니다.
- Exploding gradient 문제가 발견되면, `gradient clipping`을 사용하여 값을 제한 해볼 수 있습니다.

## LSTM(Long Short-Term Memory)

긴 시퀀스를 처리하는 RNN은 순환이 반복되면서 상대적으로 앞쪽 값의 영향이 줄어들 수 있습니다. 이를 `Long-Term Dependency`라고 합니다. LSTM 셀을 사용하면, Long-Term Dependency 문제가 완화되며 훈련 시 빠르게 수렴합니다.

<center>
  <img src={useBaseUrl('img/nn/basics/rnn/nn-rnn-lstm.jpg')} />
</center>

$$
\begin{aligned}
f_t &= \sigma(W_{fx} x_t, + W_{fh} h_{t-1} + b_f) \\
i_t &= \sigma(W_{ix} x_t, + W_{ih} h_{t-1} + b_i) \\
\widetilde{c_t} &= \tanh(W_{\tilde{c}x} x_t, + W_{\tilde{c}h} h_{t-1} + b_{\tilde{c}}) \\
o_t &= \sigma(W_{ox} x_t, + W_{oh} h_{t-1} + b_o) \\
c_t &= f_t \odot c_{t-1} + i_t \odot \widetilde{c_t} \\
h_t &= o_t \odot \tanh(c_t)
\end{aligned}
$$

- $c_t$는 `cell state`입니다.
- $h_t$는 `output`입니다.
- $f_t$는 `forget gate`로 이전 cell state를 얼마나 잊어버릴 지 결정합니다.
- $i_t$는 `input gate`로 입력 정보를 얼마나 cell state에 저장할 것일지 결정합니다.
- $o_t$는 `output gate`로 업데이트 된 cell state에서 어떤 정보를 내보낼지 결정합니다.

## GRU(Gated Recurrent Unit)
